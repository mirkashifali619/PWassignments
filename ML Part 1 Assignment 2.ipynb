{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "791227d4",
   "metadata": {},
   "source": [
    "**Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?**\n",
    "\n",
    "Overfitting: A model learns the training data too well, including noise and random fluctuations, leading to poor generalization to new data.\n",
    "Underfitting: A model has poor performance on the training data and poor generalization to new data, often due to being too simple or making overly simplified assumptions.\n",
    "\n",
    "**Consequences:**\n",
    "Overfitting: Poor performance on new data, as the model has learned the noise and doesn't generalize well.\n",
    "Underfitting:Poor performance on both the training and new data, as the model is too simple to capture the underlying patterns.\n",
    "\n",
    "**Mitigation:**\n",
    "Overfitting: Techniques such as cross-validation, regularization, using more data, and ensembling methods can be employed.\n",
    "Underfitting: Addressing underfitting involves using more complex models, enhancing feature representation, and reducing regularization.\n",
    "\n",
    "**Q2: How can we reduce overfitting? Explain in brief.**\n",
    "\n",
    "Reducing overfitting can be achieved by using techniques such as cross-validation, regularization, using more data, and ensembling methods.\n",
    "\n",
    "**Q3: Explain underfitting. List scenarios where underfitting can occur in ML.**\n",
    "\n",
    "Underfitting occurs when a model is too simple to capture the underlying patterns in the data, leading to poor performance on both the training and new data. Scenarios where underfitting can occur in ML include using linear models when the data is nonlinear, not selecting enough features, or not handling noise in the data.\n",
    "\n",
    "**Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?**\n",
    "\n",
    "The bias-variance tradeoff is the balance between the error introduced by the bias of the model and the variance of the model. High bias can lead to underfitting, while high variance can lead to overfitting. The relationship between bias and variance is that they both affect model performance. High bias can result in underfitting, while high variance can result in overfitting.\n",
    "\n",
    "**Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?**\n",
    "\n",
    "Common methods for detecting overfitting and underfitting include analyzing training and validation loss, using cross-validation, and examining the model's performance on new data. To determine whether your model is overfitting or underfitting, you can analyze the model's performance on both training and validation datasets, as well as on new data. If the model has high performance on the training data but poor performance on the validation and new data, it may be overfitting. On the other hand, if the model has poor performance on both the training and validation data, it may be underfitting.\n",
    "\n",
    "**Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?**\n",
    "\n",
    "High bias models, such as linear models, are associated with underfitting, while high variance models, like decision trees, are prone to overfitting. They differ in terms of their generalization to new data and performance.\n",
    "\n",
    "**Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.**\n",
    "\n",
    "Regularization is a technique used to prevent overfitting by adding a penalty for complexity to the model. Common techniques include L1 (Lasso) and L2 (Ridge) regularization, which constrain the model's weights. These techniques help to reduce the model's complexity and prevent overfitting by penalizing the weights of less important features or model components.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
